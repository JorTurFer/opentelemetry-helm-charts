collectors:
  daemon:
    presets:
      clusterMetrics:
        enabled: false
      kubernetesEvents:
        enabled: false

  cluster:
    suffix: cluster-stats
    replicas: 1
    mode: deployment
    enabled: true
    resources:
      limits:
        cpu: 200m
        memory: 500Mi
      requests:
        cpu: 100m
        memory: 250Mi
    presets:
      kubernetesAttributes:
        enabled: true
      kubernetesEvents:
        enabled: true
        disableLeaderElection: true
      clusterMetrics:
        enabled: true
        disableLeaderElection: true
    config:
      receivers: {}
      processors:
        batch:
          send_batch_size: 1000
          timeout: 1s
          send_batch_max_size: 1500
        resourcedetection/env:
          detectors: [env]
          timeout: 2s
          override: false
      exporters:
        debug: {}
      service:
        pipelines:
          metrics:
            receivers: [k8s_cluster]
            processors: [resourcedetection/env, batch]
            exporters: [debug]
          logs:
            receivers: [k8sobjects]
            processors: [resourcedetection/env, batch]
            exporters: [debug]

# Cluster role configuration
clusterRole:
  # Whether the cluster role is enabled or not
  enabled: true

  # Annotations for the cluster role
  annotations: {}

  # Rules for the cluster role
  rules: []

# Instrumentation configuration
instrumentation:
  # Whether instrumentation is enabled or not
  enabled: false
  labels: {}
  annotations: {}

  # Exporter configuration
  exporter:
    # This is the default collector's service
    # Upon creation of a tracing collector, edit this endpoint.
    endpoint: http://collector-collector:4317

  # Resource configuration
  resource:
    resourceAttributes: {}
    # environment: dev
    addK8sUIDAttributes: true

  # Propagators configuration
  propagators:
    - tracecontext
    - baggage
    - b3
    - b3multi
    - jaeger
    - xray
    - ottrace

  # Sampler configuration
  sampler: {}
  # type: parentbased_always_on
  # argument: "0.25"

  # Environment variables for instrumentation
  env: []
  # - name: ENV_VAR1
  #   value: value1
  # - name: ENV_VAR2
  #   value: value2

  # Java agent configuration
  java: {}
  # image: myregistry/java-agent:latest
  # volumeLimitSize: 200Mi
  # env:
  #   - name: JAVA_ENV_VAR
  #     value: java_value
  # resources:
  #   requests:
  #     memory: "64Mi"
  #     cpu: "250m"
  #   limits:
  #     memory: "128Mi"
  #     cpu: "500m"

  # NodeJS agent configuration
  nodejs: {}
  # image: myregistry/nodejs-agent:latest
  # volumeLimitSize: 200Mi
  # env:
  #   - name: NODEJS_ENV_VAR
  #     value: nodejs_value
  # resourceRequirements:
  #   requests:
  #     memory: "64Mi"
  #     cpu: "250m"
  #   limits:
  #     memory: "128Mi"
  #     cpu: "500m"

  # Python agent configuration
  python: {}
  # image: myregistry/python-agent:latest
  # volumeLimitSize: 200Mi
  # env:
  # - name: PYTHON_ENV_VAR
  #   value: python_value
  # #  Required if endpoint is set to 4317.
  # #  Python autoinstrumentation uses http/proto by default
  # #  so data must be sent to 4318 instead of 4317.
  # - name: OTEL_EXPORTER_OTLP_ENDPOINT
  #   value: http://otel-collector:4318
  # resourceRequirements:
  #   requests:
  #     memory: "64Mi"
  #     cpu: "250m"
  #   limits:
  #     memory: "128Mi"
  #     cpu: "500m"

  # .NET agent configuration
  dotnet: {}
  # image: myregistry/dotnet-agent:latest
  # volumeLimitSize: 200Mi
  # env:
  #   - name: DOTNET_ENV_VAR
  #     value: dotnet_value
  #   # Required if endpoint is set to 4317.
  #   # Dotnet autoinstrumentation uses http/proto by default
  #   # See https://github.com/open-telemetry/opentelemetry-dotnet-instrumentation/blob/888e2cd216c77d12e56b54ee91dafbc4e7452a52/docs/config.md#otlp
  #   - name: OTEL_EXPORTER_OTLP_ENDPOINT
  #     value: http://otel-collector:4318
  # resourceRequirements:
  #   requests:
  #     memory: "64Mi"
  #     cpu: "250m"
  #   limits:
  #     memory: "128Mi"
  #     cpu: "500m"

  # Go agent configuration
  go: {}
  # image: myregistry/go-agent:latest
  # volumeLimitSize: 200Mi
  # env:
  #   - name: GO_ENV_VAR
  #     value: go_value
  #   # Required if endpoint is set to 4317.
  #   # Dotnet autoinstrumentation uses http/proto by default
  #   # See https://github.com/open-telemetry/opentelemetry-dotnet-instrumentation/blob/888e2cd216c77d12e56b54ee91dafbc4e7452a52/docs/config.md#otlp
  #   - name: OTEL_EXPORTER_OTLP_ENDPOINT
  #     value: http://otel-collector:4318
  # resourceRequirements:
  #   requests:
  #     memory: "64Mi"
  #     cpu: "250m"
  #   limits:
  #     memory: "128Mi"
  #     cpu: "500m"

  # Apache HTTPd agent configuration
  apacheHttpd: {}
  # image: myregistry/apache-agent:latest
  # volumeLimitSize: 200Mi
  # env:
  #   - name: APACHE_ENV_VAR
  #     value: apache_value
  # attrs:
  #   - name: ATTRIBUTE_VAR
  #     value: attribute_value
  # version: "2.4"
  # configPath: "/usr/local/apache2/conf"
  # resourceRequirements:
  #   requests:
  #     memory: "64Mi"
  #     cpu: "250m"
  #   limits:
  #     memory: "128Mi"
  #     cpu: "500m"

  # NGINX agent configuration
  nginx: {}
  # image: myregistry/nginx-agent:latest
  # volumeLimitSize: 200Mi
  # env:
  #   - name: NGINX_ENV_VAR
  #     value: nginx_value
  # attrs:
  #   - name: ATTRIBUTE_VAR
  #     value: attribute_value
  # configFile: "/etc/nginx/nginx.conf"
  # resourceRequirements:
  #   requests:
  #     memory: "64Mi"
  #     cpu: "250m"
  #   limits:
  #     memory: "128Mi"
  #     cpu: "500m"

# OpAMP bridge configuration. The OpAMP Bridge is an OpenTelemetry component
# that enables enhanced configuration and health monitoring for OpenTelemetry collectors
# deployed in Kubernetes. The Bridge pulls collector CRDs from the Kubernetes cluster and
# reports their configuration and status to a remote OpAMP Server. The Bridge will only pull
# collectors labeled with either
# * opentelemetry.io/opamp-reporting: true
# * opentelemetry.io/opamp-managed: true
# You can learn more about the Bridge's design here:
# https://docs.google.com/document/d/1M8VLNe_sv1MIfu5bUR5OV_vrMBnAI7IJN-7-IAr37JY
opAMPBridge:
  # Whether OpAMP bridge is enabled or not
  enabled: false

  # Adds `opentelemetry.io/opamp-reporting: true` to all collectors
  addReportingLabel: true
  # Adds `opentelemetry.io/opamp-managed: true` to all collectors
  addManagedLabel: false

  # Endpoint for OpAMP server
  endpoint: http://opamp-server:8080

  # Description for additional information like non_identifying_attributes
  description: {}

  # Headers configuration for OpAMP bridge
  headers: {}
  # Authorization: Bearer your_access_token
  # Custom-Header: Custom-Value

  # Capabilities of OpAMP bridge
  # You can learn more about OpAMP's capabilities here:
  # https://github.com/open-telemetry/opamp-spec/blob/main/specification.md#agenttoservercapabilities
  capabilities:
    AcceptsOpAMPConnectionSettings: true
    AcceptsOtherConnectionSettings: true
    AcceptsRemoteConfig: true
    AcceptsRestartCommand: true
    ReportsEffectiveConfig: true
    ReportsHealth: true
    ReportsOwnLogs: true
    ReportsOwnMetrics: true
    ReportsOwnTraces: true
    ReportsRemoteConfig: true
    ReportsStatus: true

  # Components allowed for OpAMP bridge
  componentsAllowed: {}
  # receiver:
  #   - otlp
  #   - prometheus
  # processor:
  #   - batch
  #   - memory_limiter
  # exporter:
  #   - prometheusremotewrite

  # Resources configuration for OpAMP bridge
  resources:
    limits:
      cpu: "250m"
      memory: "256Mi"
    requests:
      cpu: "250m"
      memory: "256Mi"

  # Security context for OpAMP bridge
  securityContext:
    runAsNonRoot: true
    runAsUser: 1000

  # Pod security context for OpAMP bridge
  podSecurityContext:
    fsGroup: 1000

  # Pod annotations for OpAMP bridge
  podAnnotations: {}
  # prometheus.io/scrape: "true"
  # prometheus.io/port: "8080"

  # Service account for OpAMP bridge
  serviceAccount: ""

  # Image for OpAMP bridge
  image:
    repository: ghcr.io/open-telemetry/opentelemetry-operator/operator-opamp-bridge
    pullPolicy: IfNotPresent
    # By default, the version set for the bridge will match the version of the operator being run.
    tag: ""
    # When digest is set to a non-empty value, images will be pulled by digest (regardless of tag value).
    digest: ""

  # Upgrade strategy for OpAMP bridge
  upgradeStrategy: automatic

  # Volume mounts for OpAMP bridge
  volumeMounts: []
  # - name: data
  #   mountPath: /data

  # Ports configuration for OpAMP bridge
  ports: []
  # - name: http
  #   port: 8080
  #   protocol: TCP

  # Environment variables for OpAMP bridge
  env: []
  # - name: ENVIRONMENT
  #   value: production

  # Environment variables from config map for OpAMP bridge
  envFrom: []
  # - configMapRef:
  #     name: opamp-config

  # Tolerations for OpAMP bridge
  tolerations: []
  # - key: "opamp"
  #   operator: "Equal"
  #   value: "true"
  #   effect: "NoSchedule"

  # Volumes for OpAMP bridge
  volumes: []
  # - name: data
  #   emptyDir: {}

  # Whether to use host network for OpAMP bridge
  hostNetwork: false

  # Priority class name for OpAMP bridge
  priorityClassName: ""

  # Affinity configuration for OpAMP bridge
  affinity: {}
  # nodeAffinity:
  #   requiredDuringSchedulingIgnoredDuringExecution:
  #     nodeSelectorTerms:
  #       - matchExpressions:
  #           - key: opamp
  #             operator: In
  #             values:
  #               - "true"

  # Topology spread constraints for OpAMP bridge
  topologySpreadConstraints: []
  # - maxSkew: 1
  #   topologyKey: "kubernetes.io/hostname"
  #   whenUnsatisfiable: "DoNotSchedule"
  #   labelSelector:
  #     matchLabels:
  #       opamp: "true"

  # Bridge cluster role configuration
  # In order to function the bridge is given its default role to
  # list and get pods and opentelemetry collectors
  clusterRole:
    # Whether the bridge cluster role is enabled or not
    enabled: true

    # Annotations for the bridge cluster role
    annotations: {}

    # Rules for the bridge cluster role
    rules: []

############################
# Prometheus Configuration #
# (optional)               #
############################
# This configuration sections allows for a direct replacement of the kube-prometheus-stack
# chart where the collector scrapes the same metrics as the default prometheus installation.

## Flag to disable all the kubernetes component scrapers
##
kubernetesServiceMonitors:
  enabled: false
  ignoreNamespaceSelectors: false

## Component scraping the kube api server
##
kubeApiServer:
  enabled: false
  tlsConfig:
    serverName: kubernetes
    insecureSkipVerify: false
  serviceMonitor:
    ## Scrape interval. If not set, the Prometheus default scrape interval is used.
    ##
    interval: ""

    ## SampleLimit defines per-scrape limit on number of scraped samples that will be accepted.
    ##
    sampleLimit: 0

    ## TargetLimit defines a limit on the number of scraped targets that will be accepted.
    ##
    targetLimit: 0

    ## Per-scrape limit on number of labels that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
    ##
    labelLimit: 0

    ## Per-scrape limit on length of labels name that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
    ##
    labelNameLengthLimit: 0

    ## Per-scrape limit on length of labels value that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
    ##
    labelValueLengthLimit: 0

    ## proxyUrl: URL of a proxy that should be used for scraping.
    ##
    proxyUrl: ""

    jobLabel: component
    selector:
      matchLabels:
        component: apiserver
        provider: kubernetes

    ## MetricRelabelConfigs to apply to samples after scraping, but before ingestion.
    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig
    ##
    metricRelabelings:
      # Drop excessively noisy apiserver buckets.
      - action: drop
        regex: apiserver_request_duration_seconds_bucket;(0.15|0.2|0.3|0.35|0.4|0.45|0.6|0.7|0.8|0.9|1.25|1.5|1.75|2|3|3.5|4|4.5|6|7|8|9|15|25|40|50)
        sourceLabels:
          - __name__
          - le
    # - action: keep
    #   regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+'
    #   sourceLabels: [__name__]

    ## RelabelConfigs to apply to samples before scraping
    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig
    ##
    relabelings: []
    # - sourceLabels:
    #     - __meta_kubernetes_namespace
    #     - __meta_kubernetes_service_name
    #     - __meta_kubernetes_endpoint_port_name
    #   action: keep
    #   regex: default;kubernetes;https
    # - targetLabel: __address__
    #   replacement: kubernetes.default.svc:443

    ## Additional labels
    ##
    additionalLabels: {}
    #  foo: bar

## Component scraping the kubelet and kubelet-hosted cAdvisor
## the configuration for this is currently only in kubelet_scrape_configs.yaml
## This is because kubelet doesn't have a service and can only be scraped manually.
kubelet:
  enabled: false
  namespace: kube-system

  serviceMonitor:
    ## Scrape interval. If not set, the Prometheus default scrape interval is used.
    ##
    interval: ""

    ## If true, Prometheus use (respect) labels provided by exporter.
    ##
    honorLabels: true

    ## If true, Prometheus ingests metrics with timestamp provided by exporter. If false, Prometheus ingests metrics with timestamp of scrape.
    ##
    honorTimestamps: true

    ## Enable scraping the kubelet over https. For requirements to enable this see
    ## https://github.com/prometheus-operator/prometheus-operator/issues/926
    ##
    https: true

    ## Enable scraping /metrics/cadvisor from kubelet's service
    ##
    cAdvisor: true

    ## Enable scraping /metrics/probes from kubelet's service
    ##
    probes: true

## Component scraping the kube controller manager
##
kubeControllerManager:
  enabled: false

  ## If your kube controller manager is not deployed as a pod, specify IPs it can be found on
  ##
  endpoints: []
  # - 10.141.4.22
  # - 10.141.4.23
  # - 10.141.4.24

  ## If using kubeControllerManager.endpoints only the port and targetPort are used
  ##
  service:
    enabled: true
    ## If null or unset, the value is determined dynamically based on target Kubernetes version due to change
    ## of default port in Kubernetes 1.22.
    ##
    port: null
    targetPort: null
    # selector:
    #   component: kube-controller-manager

  serviceMonitor:
    enabled: true
    ## Scrape interval. If not set, the Prometheus default scrape interval is used.
    ##
    interval: ""

    ## SampleLimit defines per-scrape limit on number of scraped samples that will be accepted.
    ##
    sampleLimit: 0

    ## TargetLimit defines a limit on the number of scraped targets that will be accepted.
    ##
    targetLimit: 0

    ## Per-scrape limit on number of labels that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
    ##
    labelLimit: 0

    ## Per-scrape limit on length of labels name that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
    ##
    labelNameLengthLimit: 0

    ## Per-scrape limit on length of labels value that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
    ##
    labelValueLengthLimit: 0

    ## proxyUrl: URL of a proxy that should be used for scraping.
    ##
    proxyUrl: ""

    ## port: Name of the port the metrics will be scraped from
    ##
    port: http-metrics

    jobLabel: jobLabel
    selector: {}
    #  matchLabels:
    #    component: kube-controller-manager

    ## Enable scraping kube-controller-manager over https.
    ## Requires proper certs (not self-signed) and delegated authentication/authorization checks.
    ## If null or unset, the value is determined dynamically based on target Kubernetes version.
    ##
    https: null

    # Skip TLS certificate validation when scraping
    insecureSkipVerify: null

    # Name of the server to use when validating TLS certificate
    serverName: null

    ## MetricRelabelConfigs to apply to samples after scraping, but before ingestion.
    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig
    ##
    metricRelabelings: []
    # - action: keep
    #   regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+'
    #   sourceLabels: [__name__]

    ## RelabelConfigs to apply to samples before scraping
    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig
    ##
    relabelings: []
    # - sourceLabels: [__meta_kubernetes_pod_node_name]
    #   separator: ;
    #   regex: ^(.*)$
    #   targetLabel: nodename
    #   replacement: $1
    #   action: replace

    ## Additional labels
    ##
    additionalLabels: {}
    #  foo: bar

## Component scraping coreDns. Use either this or kubeDns
##
coreDns:
  enabled: false
  endpoints: []
  service:
    enabled: true
    port: 9153
    targetPort: 9153
    # selector:
    #   k8s-app: kube-dns
  serviceMonitor:
    enabled: true
    ## Scrape interval. If not set, the Prometheus default scrape interval is used.
    ##
    interval: ""

    ## SampleLimit defines per-scrape limit on number of scraped samples that will be accepted.
    ##
    sampleLimit: 0

    ## TargetLimit defines a limit on the number of scraped targets that will be accepted.
    ##
    targetLimit: 0

    ## Per-scrape limit on number of labels that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
    ##
    labelLimit: 0

    ## Per-scrape limit on length of labels name that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
    ##
    labelNameLengthLimit: 0

    ## Per-scrape limit on length of labels value that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
    ##
    labelValueLengthLimit: 0

    ## proxyUrl: URL of a proxy that should be used for scraping.
    ##
    proxyUrl: ""

    ## port: Name of the port the metrics will be scraped from
    ##
    port: http-metrics

    jobLabel: jobLabel
    selector: {}
    #  matchLabels:
    #    k8s-app: kube-dns

    ## MetricRelabelConfigs to apply to samples after scraping, but before ingestion.
    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig
    ##
    metricRelabelings: []
    # - action: keep
    #   regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+'
    #   sourceLabels: [__name__]

    ## RelabelConfigs to apply to samples before scraping
    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig
    ##
    relabelings: []
    # - sourceLabels: [__meta_kubernetes_pod_node_name]
    #   separator: ;
    #   regex: ^(.*)$
    #   targetLabel: nodename
    #   replacement: $1
    #   action: replace

    ## Additional labels
    ##
    additionalLabels: {}
    #  foo: bar

## Component scraping kubeDns. Use either this or coreDns
##
kubeDns:
  enabled: false
  service:
    dnsmasq:
      port: 10054
      targetPort: 10054
    skydns:
      port: 10055
      targetPort: 10055
    # selector:
    #   k8s-app: kube-dns
  serviceMonitor:
    ## Scrape interval. If not set, the Prometheus default scrape interval is used.
    ##
    interval: ""

    ## SampleLimit defines per-scrape limit on number of scraped samples that will be accepted.
    ##
    sampleLimit: 0

    ## TargetLimit defines a limit on the number of scraped targets that will be accepted.
    ##
    targetLimit: 0

    ## Per-scrape limit on number of labels that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
    ##
    labelLimit: 0

    ## Per-scrape limit on length of labels name that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
    ##
    labelNameLengthLimit: 0

    ## Per-scrape limit on length of labels value that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
    ##
    labelValueLengthLimit: 0

    ## proxyUrl: URL of a proxy that should be used for scraping.
    ##
    proxyUrl: ""

    jobLabel: jobLabel
    selector: {}
    #  matchLabels:
    #    k8s-app: kube-dns

    ## MetricRelabelConfigs to apply to samples after scraping, but before ingestion.
    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig
    ##
    metricRelabelings: []
    # - action: keep
    #   regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+'
    #   sourceLabels: [__name__]

    ## RelabelConfigs to apply to samples before scraping
    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig
    ##
    relabelings: []
    # - sourceLabels: [__meta_kubernetes_pod_node_name]
    #   separator: ;
    #   regex: ^(.*)$
    #   targetLabel: nodename
    #   replacement: $1
    #   action: replace

    ## MetricRelabelConfigs to apply to samples after scraping, but before ingestion.
    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig
    ##
    dnsmasqMetricRelabelings: []
    # - action: keep
    #   regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+'
    #   sourceLabels: [__name__]

    ## RelabelConfigs to apply to samples before scraping
    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig
    ##
    dnsmasqRelabelings: []
    # - sourceLabels: [__meta_kubernetes_pod_node_name]
    #   separator: ;
    #   regex: ^(.*)$
    #   targetLabel: nodename
    #   replacement: $1
    #   action: replace

    ## Additional labels
    ##
    additionalLabels: {}
    #  foo: bar

## Component scraping etcd
##
kubeEtcd:
  enabled: false

  ## If your etcd is not deployed as a pod, specify IPs it can be found on
  ##
  endpoints: []
  # - 10.141.4.22
  # - 10.141.4.23
  # - 10.141.4.24

  ## Etcd service. If using kubeEtcd.endpoints only the port and targetPort are used
  ##
  service:
    enabled: true
    port: 2381
    targetPort: 2381
    # selector:
    #   component: etcd

  ## Configure secure access to the etcd cluster by loading a secret into prometheus and
  ## specifying security configuration below. For example, with a secret named etcd-client-cert
  ##
  ## serviceMonitor:
  ##   scheme: https
  ##   insecureSkipVerify: false
  ##   serverName: localhost
  ##   caFile: /etc/prometheus/secrets/etcd-client-cert/etcd-ca
  ##   certFile: /etc/prometheus/secrets/etcd-client-cert/etcd-client
  ##   keyFile: /etc/prometheus/secrets/etcd-client-cert/etcd-client-key
  ##
  serviceMonitor:
    enabled: true
    ## Scrape interval. If not set, the Prometheus default scrape interval is used.
    ##
    interval: ""

    ## SampleLimit defines per-scrape limit on number of scraped samples that will be accepted.
    ##
    sampleLimit: 0

    ## TargetLimit defines a limit on the number of scraped targets that will be accepted.
    ##
    targetLimit: 0

    ## Per-scrape limit on number of labels that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
    ##
    labelLimit: 0

    ## Per-scrape limit on length of labels name that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
    ##
    labelNameLengthLimit: 0

    ## Per-scrape limit on length of labels value that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
    ##
    labelValueLengthLimit: 0

    ## proxyUrl: URL of a proxy that should be used for scraping.
    ##
    proxyUrl: ""
    scheme: http
    insecureSkipVerify: false
    serverName: ""
    caFile: ""
    certFile: ""
    keyFile: ""

    ## port: Name of the port the metrics will be scraped from
    ##
    port: http-metrics

    jobLabel: jobLabel
    selector: {}
    #  matchLabels:
    #    component: etcd

    ## MetricRelabelConfigs to apply to samples after scraping, but before ingestion.
    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig
    ##
    metricRelabelings: []
    # - action: keep
    #   regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+'
    #   sourceLabels: [__name__]

    ## RelabelConfigs to apply to samples before scraping
    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig
    ##
    relabelings: []
    # - sourceLabels: [__meta_kubernetes_pod_node_name]
    #   separator: ;
    #   regex: ^(.*)$
    #   targetLabel: nodename
    #   replacement: $1
    #   action: replace

    ## Additional labels
    ##
    additionalLabels: {}
    #  foo: bar

## Component scraping kube scheduler
##
kubeScheduler:
  enabled: false

  ## If your kube scheduler is not deployed as a pod, specify IPs it can be found on
  ##
  endpoints: []
  # - 10.141.4.22
  # - 10.141.4.23
  # - 10.141.4.24

  ## If using kubeScheduler.endpoints only the port and targetPort are used
  ##
  service:
    enabled: true
    ## If null or unset, the value is determined dynamically based on target Kubernetes version due to change
    ## of default port in Kubernetes 1.23.
    ##
    port: null
    targetPort: null
    # selector:
    #   component: kube-scheduler

  serviceMonitor:
    enabled: true
    ## Scrape interval. If not set, the Prometheus default scrape interval is used.
    ##
    interval: ""

    ## SampleLimit defines per-scrape limit on number of scraped samples that will be accepted.
    ##
    sampleLimit: 0

    ## TargetLimit defines a limit on the number of scraped targets that will be accepted.
    ##
    targetLimit: 0

    ## Per-scrape limit on number of labels that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
    ##
    labelLimit: 0

    ## Per-scrape limit on length of labels name that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
    ##
    labelNameLengthLimit: 0

    ## Per-scrape limit on length of labels value that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
    ##
    labelValueLengthLimit: 0

    ## proxyUrl: URL of a proxy that should be used for scraping.
    ##
    proxyUrl: ""
    ## Enable scraping kube-scheduler over https.
    ## Requires proper certs (not self-signed) and delegated authentication/authorization checks.
    ## If null or unset, the value is determined dynamically based on target Kubernetes version.
    ##
    https: null

    ## port: Name of the port the metrics will be scraped from
    ##
    port: http-metrics

    jobLabel: jobLabel
    selector: {}
    #  matchLabels:
    #    component: kube-scheduler

    ## Skip TLS certificate validation when scraping
    insecureSkipVerify: null

    ## Name of the server to use when validating TLS certificate
    serverName: null

    ## MetricRelabelConfigs to apply to samples after scraping, but before ingestion.
    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig
    ##
    metricRelabelings: []
    # - action: keep
    #   regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+'
    #   sourceLabels: [__name__]

    ## RelabelConfigs to apply to samples before scraping
    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig
    ##
    relabelings: []
    # - sourceLabels: [__meta_kubernetes_pod_node_name]
    #   separator: ;
    #   regex: ^(.*)$
    #   targetLabel: nodename
    #   replacement: $1
    #   action: replace

    ## Additional labels
    ##
    additionalLabels: {}
    #  foo: bar

## Component scraping kube proxy
##
kubeProxy:
  enabled: false

  ## If your kube proxy is not deployed as a pod, specify IPs it can be found on
  ##
  endpoints: []
  # - 10.141.4.22
  # - 10.141.4.23
  # - 10.141.4.24

  service:
    enabled: true
    port: 10249
    targetPort: 10249
    # selector:
    #   k8s-app: kube-proxy

  serviceMonitor:
    enabled: true
    ## Scrape interval. If not set, the Prometheus default scrape interval is used.
    ##
    interval: ""

    ## SampleLimit defines per-scrape limit on number of scraped samples that will be accepted.
    ##
    sampleLimit: 0

    ## TargetLimit defines a limit on the number of scraped targets that will be accepted.
    ##
    targetLimit: 0

    ## Per-scrape limit on number of labels that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
    ##
    labelLimit: 0

    ## Per-scrape limit on length of labels name that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
    ##
    labelNameLengthLimit: 0

    ## Per-scrape limit on length of labels value that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
    ##
    labelValueLengthLimit: 0

    ## proxyUrl: URL of a proxy that should be used for scraping.
    ##
    proxyUrl: ""

    ## port: Name of the port the metrics will be scraped from
    ##
    port: http-metrics

    jobLabel: jobLabel
    selector: {}
    #  matchLabels:
    #    k8s-app: kube-proxy

    ## Enable scraping kube-proxy over https.
    ## Requires proper certs (not self-signed) and delegated authentication/authorization checks
    ##
    https: false

    ## MetricRelabelConfigs to apply to samples after scraping, but before ingestion.
    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig
    ##
    metricRelabelings: []
    # - action: keep
    #   regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+'
    #   sourceLabels: [__name__]

    ## RelabelConfigs to apply to samples before scraping
    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig
    ##
    relabelings: []
    # - action: keep
    #   regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+'
    #   sourceLabels: [__name__]

    ## Additional labels
    ##
    additionalLabels: {}
    #  foo: bar

## Controls whether the kube-state-metrics chart should be created.
## This block matches the configuration for the kube-prometheus-stack chart for compatibility.
kubeStateMetrics:
  enabled: false

## Configuration for kube-state-metrics subchart
## The Kube-State-Metrics agent collects cluster-level metrics
## Read more here about the chart:
## https://github.com/prometheus-community/helm-charts/tree/main/charts/kube-state-metrics
kube-state-metrics:
  namespaceOverride: ""
  rbac:
    create: true
  releaseLabel: true
  prometheus:
    monitor:
      enabled: true

      ## Scrape interval. If not set, the Prometheus default scrape interval is used.
      ##
      interval: ""

      ## SampleLimit defines per-scrape limit on number of scraped samples that will be accepted.
      ##
      sampleLimit: 0

      ## TargetLimit defines a limit on the number of scraped targets that will be accepted.
      ##
      targetLimit: 0

      ## Per-scrape limit on number of labels that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
      ##
      labelLimit: 0

      ## Per-scrape limit on length of labels name that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
      ##
      labelNameLengthLimit: 0

      ## Per-scrape limit on length of labels value that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
      ##
      labelValueLengthLimit: 0

      ## Scrape Timeout. If not set, the Prometheus default scrape timeout is used.
      ##
      scrapeTimeout: ""

      ## proxyUrl: URL of a proxy that should be used for scraping.
      ##
      proxyUrl: ""

      # Keep labels from scraped data, overriding server-side labels
      ##
      honorLabels: true

      ## MetricRelabelConfigs to apply to samples after scraping, but before ingestion.
      ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig
      ##
      metricRelabelings: []
      # - action: keep
      #   regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+'
      #   sourceLabels: [__name__]

      ## RelabelConfigs to apply to samples before scraping
      ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig
      ##
      relabelings: []
      # - sourceLabels: [__meta_kubernetes_pod_node_name]
      #   separator: ;
      #   regex: ^(.*)$
      #   targetLabel: nodename
      #   replacement: $1
      #   action: replace

  selfMonitor:
    enabled: false

## Controls whether the prometheus-node-exporter chart should be created.
## This block matches the configuration for the kube-prometheus-stack chart for compatibility.
nodeExporter:
  enabled: false

## Configuration for prometheus-node-exporter subchart
## This will install a daemonset that pulls metric data from each node
## Read more here about the chart:
## https://github.com/prometheus-community/helm-charts/tree/main/charts/prometheus-node-exporter
prometheus-node-exporter:
  namespaceOverride: ""
  podLabels:
    ## Add the 'node-exporter' label to be used by serviceMonitor to match standard common usage in rules and grafana dashboards
    ##
    jobLabel: node-exporter
  releaseLabel: true
  extraArgs:
    - --collector.filesystem.mount-points-exclude=^/(dev|proc|sys|var/lib/docker/.+|var/lib/kubelet/.+)($|/)
    - --collector.filesystem.fs-types-exclude=^(autofs|binfmt_misc|bpf|cgroup2?|configfs|debugfs|devpts|devtmpfs|fusectl|hugetlbfs|iso9660|mqueue|nsfs|overlay|proc|procfs|pstore|rpc_pipefs|securityfs|selinuxfs|squashfs|sysfs|tracefs)$
  service:
    portName: http-metrics
  prometheus:
    monitor:
      enabled: true

      jobLabel: jobLabel

      ## Scrape interval. If not set, the Prometheus default scrape interval is used.
      ##
      interval: ""

      ## SampleLimit defines per-scrape limit on number of scraped samples that will be accepted.
      ##
      sampleLimit: 0

      ## TargetLimit defines a limit on the number of scraped targets that will be accepted.
      ##
      targetLimit: 0

      ## Per-scrape limit on number of labels that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
      ##
      labelLimit: 0

      ## Per-scrape limit on length of labels name that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
      ##
      labelNameLengthLimit: 0

      ## Per-scrape limit on length of labels value that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
      ##
      labelValueLengthLimit: 0

      ## How long until a scrape request times out. If not set, the Prometheus default scape timeout is used.
      ##
      scrapeTimeout: ""

      ## proxyUrl: URL of a proxy that should be used for scraping.
      ##
      proxyUrl: ""

      ## MetricRelabelConfigs to apply to samples after scraping, but before ingestion.
      ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig
      ##
      metricRelabelings: []
      # - sourceLabels: [__name__]
      #   separator: ;
      #   regex: ^node_mountstats_nfs_(event|operations|transport)_.+
      #   replacement: $1
      #   action: drop

      ## RelabelConfigs to apply to samples before scraping
      ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig
      ##
      relabelings: []
      # - sourceLabels: [__meta_kubernetes_pod_node_name]
      #   separator: ;
      #   regex: ^(.*)$
      #   targetLabel: nodename
      #   replacement: $1
      #   action: replace
  rbac:
    ## If true, create PSPs for node-exporter
    ##
    pspEnabled: false
